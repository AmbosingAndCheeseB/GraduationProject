# -*- coding: utf-8 -*-
"""bert_finetuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UQpDZBHskB43wnwqaPXKkY_OZ1NWtvPW
"""

from google.colab import drive
drive.mount('/content/drive/')



!ls "/content/drive/My Drive/Jolp/"

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))


import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.utils import shuffle
from pytorch_pretrained_bert import BertTokenizer, BertConfig
from pytorch_pretrained_bert import BertAdam, BertForMaskedLM
from tqdm import tqdm, trange
import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt
import re
from unidecode import unidecode
import matplotlib.pyplot as plt


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)

from google.colab import files
uploaded = files.upload()
uploaded_vali = files.upload()
uploaded_test = files.upload()

df = pd.read_csv("toeic_data_train.csv", header=0)
print(df.shape)
df2 = pd.read_csv("toeic_data_vali.csv", header = 0)
print(df2.shape)
df3 = pd.read_csv("toeic_data_test.csv", header = 0)
print(df3.shape)

def get_score(model, tokenizer, input_tensor, segment_tensor, masked_index, candidate):
  candidate_tokens = tokenizer.tokenize(candidate) 
  candidate_ids = tokenizer.convert_tokens_to_ids(candidate_tokens)
  predictions = model(input_tensor, segment_tensor)
  predictions_candidates = predictions[0, masked_index, candidate_ids].mean()
  
  del input_tensor
  del segment_tensor

  return predictions_candidates.item()

questions = df.question.values
questions = ["[CLS] " + question + " [SEP]" for question in questions]

vali_questions = df2.question.values
test_questions = df3.question.values

candi_vali = [[df2['1'][i], df2['2'][i], df2['3'][i], df2['4'][i], df2['answer'][i]] for i in range(len(df2))] 
candi_test = [[df3['1'][i], df3['2'][i], df3['3'][i], df3['4'][i], df3['answer'][i]] for i in range(len(df3))]

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

tokenized_texts = [tokenizer.tokenize(ques) for ques in questions]

tokenized_vali_texts = [tokenizer.tokenize(ques) for ques in vali_questions]
tokenized_test_texts = [tokenizer.tokenize(ques) for ques in test_questions]

vali_masked_index = [ques.index("[MASK]") for ques in tokenized_vali_texts]
test_masked_index = [ques.index("[MASK]") for ques in tokenized_test_texts]

MAX_LEN = 128
input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

vali_ids = [tokenizer.convert_tokens_to_ids(v) for v in tokenized_vali_texts]
test_ids = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test_texts]

segment_vali_ids = [[0] * len(tokenized_vali_texts[i]) for i in range(len(tokenized_vali_texts))]
segment_test_ids = [[0] * len(tokenized_test_texts[i]) for i in range(len(tokenized_test_texts))]

attention_masks = []

for seq in input_ids:
  seq_mask = [float(i>0) for i in seq]
  attention_masks.append(seq_mask)

train_inputs, train_masks = shuffle(input_ids, attention_masks)
train_inputs = torch.tensor(train_inputs)
train_masks = torch.tensor(train_masks)

batch_size = 4

train_data = TensorDataset(train_inputs, train_masks)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

model = BertForMaskedLM.from_pretrained("bert-base-uncased")
model.cuda()


param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}
]

optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-8, warmup=.1)

train_loss_set = []

epochs = 10
problem = 0
correct = 0

PATH = "/content/drive/My Drive/Jolp/param/epoch10.pt"

print("이전 모델을 불러올까요? (y/n) : ")
res1 = input()

if(res1 == "Y" or res1 ==  "y"):
 
  checkpoint = torch.load(PATH)
  model.load_state_dict(checkpoint['model_state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  epoch_ck = checkpoint['epoch']
  loss_ck = checkpoint['loss']
  print("이전 상태의 loss : %f" %(loss_ck))
  print("이전 상태의 epoch : %f" %(epoch_ck))
  model.eval()

print("모델을 학습시킬까요? (y/n) : ")
res2 = input()

if(res2 == "Y" or res2 == "y"):
    
  for _ in trange(epochs, desc = "Epoch"):

    model.train()

    tr_loss = 0
    
    for step, batch in enumerate(train_dataloader):
      batch = tuple(b.to(device) for b in batch)

      b_input_ids, b_input_mask = batch
      del batch
    
      optimizer.zero_grad()

      outputs = model(b_input_ids, attention_mask = b_input_mask, masked_lm_labels = b_input_ids)
      del b_input_ids, b_input_mask
      loss = outputs
      train_loss_set.append(loss)
      loss.backward()

      optimizer.step()


  torch.save({
  'epoch': epochs,
  'model_state_dict': model.state_dict(),
  'optimizer_state_dict': optimizer.state_dict(),
  'loss': loss,
  }, PATH)

model.eval()

for i in tqdm(range(len(vali_ids))):
  vali_input = torch.tensor([vali_ids[i]]).to(device)
  segment_vali_tensor = torch.tensor([segment_vali_ids[i]]).to(device)
  with torch.no_grad():
    logit = torch.tensor([get_score(model, tokenizer, vali_input, segment_vali_tensor, vali_masked_index[i], candi) for candi in candi_vali[i]])
    logit_idx = torch.argmax(logit).item()
  
    if(candi_vali[i][4] == candi_vali[i][logit_idx]):
      problem += 1
      correct += 1
    else:
      problem += 1


print("정답율 : %f" % ((correct/problem)*100))
plt.plot(train_loss_set)
plt.show()